{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9b068b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f504664d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_379']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.1171407699585\n",
      "Positive Review\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "example = \"my birthday is near this month\"\n",
    "deep_predict = classifier(example)\n",
    "deep_score = deep_predict[0]['score']\n",
    "print(deep_score*100)\n",
    "if deep_predict[0]['label'] == 'POSITIVE':\n",
    "    deep_predict = 'Positive Review'\n",
    "else:\n",
    "    depp_predict = 'NEGATIVE Review'\n",
    "print(deep_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a3487b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997667670249939},\n",
       " {'label': 'NEGATIVE', 'score': 0.9992707371711731},\n",
       " {'label': 'NEGATIVE', 'score': 0.9624555706977844}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(['brasil is the best team in the wolrd','argentina sucks!','yeah you are hitler'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79aa0b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A narrative paragraph tells a story, either real or fictional, by introducing a topic, giving more details, and then ending with a reflection or transition to another paragraph. whats the bag?\n"
     ]
    }
   ],
   "source": [
    "#spacy implimentation\n",
    "import spacy\n",
    "analyzer = spacy.load('en_core_web_sm')\n",
    "\n",
    "example = analyzer(\"A narrative paragraph tells a story, either real or fictional, by introducing a topic, giving more details, and then ending with a reflection or transition to another paragraph. whats the bag?\")\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3985ae3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f2f84c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A narrative paragraph tells a story, either real or fictional, by introducing a topic, giving more details, and then ending with a reflection or transition to another paragraph.\n",
      "A\n",
      "narrative\n",
      "paragraph\n",
      "tells\n",
      "a\n",
      "story\n",
      ",\n",
      "either\n",
      "real\n",
      "or\n",
      "fictional\n",
      ",\n",
      "by\n",
      "introducing\n",
      "a\n",
      "topic\n",
      ",\n",
      "giving\n",
      "more\n",
      "details\n",
      ",\n",
      "and\n",
      "then\n",
      "ending\n",
      "with\n",
      "a\n",
      "reflection\n",
      "or\n",
      "transition\n",
      "to\n",
      "another\n",
      "paragraph\n",
      ".\n",
      "whats the bag?\n",
      "what\n",
      "s\n",
      "the\n",
      "bag\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for sentence in example.sents:\n",
    "    print(sentence)\n",
    "    for words in sentence:\n",
    "        print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9069e0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py\", line 1921, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\", line 1783, in _download\n",
      "    return self._download_threaded(*e)\n",
      "  File \"C:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\downloader.py\", line 2069, in _download_threaded\n",
      "    assert self._download_msg_queue == []\n",
      "AssertionError\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce076282",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dell/nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m----> 3\u001b[0m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessi is the real hero of arg.neymar jr. is from brasil.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlanguage\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dell/nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize(\"messi is the real hero of arg.neymar jr. is from brasil.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6458d8d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A for have narrative paragraph tells a Story Either Real or @ Fictional,fire!\n",
      "narrative\n",
      "paragraph\n",
      "tell\n",
      "Story\n",
      "real\n",
      "Fictional\n",
      "fire\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'narrative paragraph tell Story real Fictional fire'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spacy implimentation\n",
    "import spacy\n",
    "analyzer = spacy.load('en_core_web_sm')\n",
    "\n",
    "example = analyzer(\"A for have narrative paragraph tells a Story Either Real or @ Fictional,fire!\")\n",
    "print(example)\n",
    "token_words = []\n",
    "processed_words = []\n",
    "for words  in example:\n",
    "    if not words.is_stop and not words.is_punct:\n",
    "        token_words.append(words)\n",
    "for new in token_words:\n",
    "    print(new.lemma_)\n",
    "    processed_words.append(new.lemma_)\n",
    "token_words.clear()    \n",
    "processed_words = \" \".join(processed_words)    \n",
    "processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "851b2836",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'is_stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m token_words:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(token)\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_stop\u001b[49m:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(token)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'is_stop'"
     ]
    }
   ],
   "source": [
    "for token in token_words:\n",
    "    print(token)\n",
    "    if token.is_stop:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c774bc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'narrative',\n",
       " 'paragraph',\n",
       " 'tell',\n",
       " 'a',\n",
       " 'Story',\n",
       " ',',\n",
       " 'either',\n",
       " 'real',\n",
       " 'or',\n",
       " 'fictional',\n",
       " ',',\n",
       " 'by',\n",
       " 'introduce',\n",
       " 'a',\n",
       " 'topic',\n",
       " ',',\n",
       " 'give',\n",
       " 'more',\n",
       " 'detail',\n",
       " ',',\n",
       " 'and',\n",
       " 'then',\n",
       " 'end',\n",
       " 'with',\n",
       " 'a',\n",
       " 'reflection',\n",
       " 'or',\n",
       " 'transition',\n",
       " 'to',\n",
       " 'another',\n",
       " 'paragraph',\n",
       " '.',\n",
       " 'what',\n",
       " 's',\n",
       " 'the',\n",
       " 'bag',\n",
       " '?']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9c8109df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brasilian good fottball study\n",
      "A for have narrative paragraph tells a Story Either Real or @ Fictional,fire!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.32652336,  0.02900009,  0.37005952,  0.12274535, -0.34736046,\n",
       "        0.12857969,  0.02573625,  0.00409023,  0.24163339, -0.02744887,\n",
       "        0.38642606, -0.39455914, -0.08317854,  0.19616096,  0.0778863 ,\n",
       "        0.03526477,  0.1834029 ,  0.1354141 ,  0.18216011, -0.03686218,\n",
       "       -0.26002917, -0.41504142, -0.1591767 ,  0.08765918, -0.28884947,\n",
       "        0.18284073, -0.02725443, -0.1332449 , -0.23852462,  0.24133755,\n",
       "       -0.17211093,  0.52289236, -0.45162234,  0.10027678, -0.11585033,\n",
       "        0.28265834, -0.45026678, -0.08192828, -0.6357385 ,  0.09870865,\n",
       "        0.32212132,  0.19926903, -0.25053087, -0.3971308 , -0.09430353,\n",
       "       -0.23335576,  0.21065313,  0.26216418, -0.08431248, -0.354104  ,\n",
       "        0.48715416, -0.20378295, -0.00201181, -0.273278  ,  0.05795886,\n",
       "       -0.15885484, -0.11038768,  0.01712018,  0.11063325,  0.00879868,\n",
       "        0.02534448, -0.08811706,  0.0882102 , -0.21215755,  0.07876319,\n",
       "       -0.42501798, -0.01893679,  0.31559968, -0.3848917 , -0.28070182,\n",
       "        0.18575513, -0.16627677,  0.22658713,  0.3128607 , -0.28261432,\n",
       "        0.16175222,  0.10512422,  0.45040056, -0.24437267, -0.05528818,\n",
       "       -0.10502888, -0.07985074,  0.3892758 ,  0.07237382,  0.30165038,\n",
       "        0.09720226,  0.3377324 ,  0.13988322, -0.22980693,  0.52385974,\n",
       "       -0.06754333, -0.725906  ,  0.09029029, -0.11130764,  0.20775166,\n",
       "       -0.15941048], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessor(text):\n",
    "    example = analyzer(text)\n",
    "    token_words = []\n",
    "    processed_words = []\n",
    "    for words  in example:\n",
    "        if not words.is_stop and not words.is_punct:\n",
    "            token_words.append(words)\n",
    "    for new in token_words:\n",
    "        processed_words.append(new.lemma_)\n",
    "    token_words.clear()    \n",
    "    return \" \".join(processed_words)   \n",
    "print(preprocessor(\"brasilian are good at fottball! and studying \"))\n",
    "new = preprocessor(\"new study is amazing\")\n",
    "print(example)\n",
    "example.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0e462b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ranumandal', 'for', '1235']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list  = []\n",
    "text = ['ranumandal','@','for','1235']\n",
    "for i in text:\n",
    "    if i.isalnum():\n",
    "        list.append(i)\n",
    "\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b8d7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
